{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import First Dataset (Volume Per Train Station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     YEAR_MONTH          DAY_TYPE  TIME_PER_HOUR PT_TYPE        PT_CODE  \\\n",
      "0       2023-08           WEEKDAY             22   TRAIN           NS28   \n",
      "1       2023-08  WEEKENDS/HOLIDAY             22   TRAIN           NS28   \n",
      "2       2023-08  WEEKENDS/HOLIDAY              0   TRAIN      DT10/TE11   \n",
      "3       2023-08           WEEKDAY              0   TRAIN      DT10/TE11   \n",
      "4       2023-08           WEEKDAY             10   TRAIN  EW16/NE3/TE17   \n",
      "...         ...               ...            ...     ...            ...   \n",
      "6815    2023-08  WEEKENDS/HOLIDAY              6   TRAIN           DT23   \n",
      "6816    2023-08  WEEKENDS/HOLIDAY              7   TRAIN  NS27/CE2/TE20   \n",
      "6817    2023-08           WEEKDAY              7   TRAIN  NS27/CE2/TE20   \n",
      "6818    2023-08  WEEKENDS/HOLIDAY             12   TRAIN            SE5   \n",
      "6819    2023-08           WEEKDAY             12   TRAIN            SE5   \n",
      "\n",
      "      TOTAL_TAP_IN_VOLUME  TOTAL_TAP_OUT_VOLUME  \n",
      "0                     752                   311  \n",
      "1                     612                   223  \n",
      "2                      37                   242  \n",
      "3                      86                   445  \n",
      "4                   28179                 39454  \n",
      "...                   ...                   ...  \n",
      "6815                  365                   400  \n",
      "6816                  476                   863  \n",
      "6817                 1283                  8802  \n",
      "6818                 2080                  1213  \n",
      "6819                 3811                  2219  \n",
      "\n",
      "[6820 rows x 7 columns]\n",
      "     YEAR_MONTH          DAY_TYPE  TIME_PER_HOUR PT_TYPE        PT_CODE  \\\n",
      "0       2023-09  WEEKENDS/HOLIDAY             14   TRAIN           CC16   \n",
      "1       2023-09           WEEKDAY             14   TRAIN           CC16   \n",
      "2       2023-09  WEEKENDS/HOLIDAY             10   TRAIN           EW11   \n",
      "3       2023-09           WEEKDAY             10   TRAIN           EW11   \n",
      "4       2023-09           WEEKDAY              6   TRAIN   NS24/NE6/CC1   \n",
      "...         ...               ...            ...     ...            ...   \n",
      "6816    2023-09           WEEKDAY              6   TRAIN           DT23   \n",
      "6817    2023-09  WEEKENDS/HOLIDAY              7   TRAIN  NS27/CE2/TE20   \n",
      "6818    2023-09           WEEKDAY              7   TRAIN  NS27/CE2/TE20   \n",
      "6819    2023-09  WEEKENDS/HOLIDAY             12   TRAIN            SE5   \n",
      "6820    2023-09           WEEKDAY             12   TRAIN            SE5   \n",
      "\n",
      "      TOTAL_TAP_IN_VOLUME  TOTAL_TAP_OUT_VOLUME  \n",
      "0                    3585                  3153  \n",
      "1                    5869                  5846  \n",
      "2                   10830                  9821  \n",
      "3                   20095                 18338  \n",
      "4                    3553                  8804  \n",
      "...                   ...                   ...  \n",
      "6816                 1917                  2946  \n",
      "6817                  427                   802  \n",
      "6818                 1175                  8034  \n",
      "6819                 2178                  1239  \n",
      "6820                 3458                  1891  \n",
      "\n",
      "[6821 rows x 7 columns]\n",
      "     YEAR_MONTH          DAY_TYPE  TIME_PER_HOUR PT_TYPE        PT_CODE  \\\n",
      "0       2023-10  WEEKENDS/HOLIDAY             11   TRAIN            NS7   \n",
      "1       2023-10           WEEKDAY             11   TRAIN            NS7   \n",
      "2       2023-10  WEEKENDS/HOLIDAY             16   TRAIN            SW4   \n",
      "3       2023-10           WEEKDAY             16   TRAIN            SW4   \n",
      "4       2023-10  WEEKENDS/HOLIDAY             10   TRAIN            CC5   \n",
      "...         ...               ...            ...     ...            ...   \n",
      "6815    2023-10  WEEKENDS/HOLIDAY              6   TRAIN           DT23   \n",
      "6816    2023-10  WEEKENDS/HOLIDAY              7   TRAIN  NS27/CE2/TE20   \n",
      "6817    2023-10           WEEKDAY              7   TRAIN  NS27/CE2/TE20   \n",
      "6818    2023-10           WEEKDAY             12   TRAIN            SE5   \n",
      "6819    2023-10  WEEKENDS/HOLIDAY             12   TRAIN            SE5   \n",
      "\n",
      "      TOTAL_TAP_IN_VOLUME  TOTAL_TAP_OUT_VOLUME  \n",
      "0                   10207                 11033  \n",
      "1                   20072                 13995  \n",
      "2                    1031                  1107  \n",
      "3                    1985                  2669  \n",
      "4                    1308                  2484  \n",
      "...                   ...                   ...  \n",
      "6815                  482                   393  \n",
      "6816                  392                   765  \n",
      "6817                 1196                  8911  \n",
      "6818                 5001                  2533  \n",
      "6819                 1763                  1030  \n",
      "\n",
      "[6820 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "augData = './datasets/Aug_2023.csv'\n",
    "septData = './datasets/Sept_2023.csv'\n",
    "octData = './datasets/Oct_2023.csv'\n",
    "\n",
    "\n",
    "dfAug = pd.read_csv(augData)\n",
    "dfSept = pd.read_csv(septData)\n",
    "dfOct = pd.read_csv(octData)\n",
    "\n",
    "print(dfAug)\n",
    "print(dfSept)\n",
    "print(dfOct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine and verify all files for first dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      YEAR_MONTH          DAY_TYPE  TIME_PER_HOUR PT_TYPE        PT_CODE  \\\n",
      "0        2023-08           WEEKDAY             22   TRAIN           NS28   \n",
      "1        2023-08  WEEKENDS/HOLIDAY             22   TRAIN           NS28   \n",
      "2        2023-08  WEEKENDS/HOLIDAY              0   TRAIN      DT10/TE11   \n",
      "3        2023-08           WEEKDAY              0   TRAIN      DT10/TE11   \n",
      "4        2023-08           WEEKDAY             10   TRAIN  EW16/NE3/TE17   \n",
      "...          ...               ...            ...     ...            ...   \n",
      "20456    2023-10  WEEKENDS/HOLIDAY              6   TRAIN           DT23   \n",
      "20457    2023-10  WEEKENDS/HOLIDAY              7   TRAIN  NS27/CE2/TE20   \n",
      "20458    2023-10           WEEKDAY              7   TRAIN  NS27/CE2/TE20   \n",
      "20459    2023-10           WEEKDAY             12   TRAIN            SE5   \n",
      "20460    2023-10  WEEKENDS/HOLIDAY             12   TRAIN            SE5   \n",
      "\n",
      "       TOTAL_TAP_IN_VOLUME  TOTAL_TAP_OUT_VOLUME  \n",
      "0                      752                   311  \n",
      "1                      612                   223  \n",
      "2                       37                   242  \n",
      "3                       86                   445  \n",
      "4                    28179                 39454  \n",
      "...                    ...                   ...  \n",
      "20456                  482                   393  \n",
      "20457                  392                   765  \n",
      "20458                 1196                  8911  \n",
      "20459                 5001                  2533  \n",
      "20460                 1763                  1030  \n",
      "\n",
      "[20461 rows x 7 columns]\n",
      "Total Rows should be: 20461\n",
      "Number of columns is the same for all three DataFrames: 7\n"
     ]
    }
   ],
   "source": [
    "# Combine All DataFrames\n",
    "dfCombinedFirst = pd.concat([dfAug, dfSept, dfOct], ignore_index=True)\n",
    "print(dfCombinedFirst)\n",
    "\n",
    "# To Verify Combination\n",
    "totalRows = dfAug.shape[0] + dfSept.shape[0] + dfOct.shape[0]\n",
    "print(\"Total Rows should be:\", totalRows)\n",
    "if dfAug.shape[1] == dfSept.shape[1] == dfOct.shape[1]:\n",
    "    print(\"Number of columns is the same for all three DataFrames:\", dfAug.shape[1])\n",
    "else:\n",
    "    print(\"Column size is not the same for the DataFrames\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Our First Dataset for Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Blank Columns or Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       YEAR_MONTH  DAY_TYPE  TIME_PER_HOUR  PT_TYPE  PT_CODE  \\\n",
      "0           False     False          False    False    False   \n",
      "1           False     False          False    False    False   \n",
      "2           False     False          False    False    False   \n",
      "3           False     False          False    False    False   \n",
      "4           False     False          False    False    False   \n",
      "...           ...       ...            ...      ...      ...   \n",
      "20456       False     False          False    False    False   \n",
      "20457       False     False          False    False    False   \n",
      "20458       False     False          False    False    False   \n",
      "20459       False     False          False    False    False   \n",
      "20460       False     False          False    False    False   \n",
      "\n",
      "       TOTAL_TAP_IN_VOLUME  TOTAL_TAP_OUT_VOLUME  \n",
      "0                    False                 False  \n",
      "1                    False                 False  \n",
      "2                    False                 False  \n",
      "3                    False                 False  \n",
      "4                    False                 False  \n",
      "...                    ...                   ...  \n",
      "20456                False                 False  \n",
      "20457                False                 False  \n",
      "20458                False                 False  \n",
      "20459                False                 False  \n",
      "20460                False                 False  \n",
      "\n",
      "[20461 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Using isna() method to find blanks\n",
    "blanks = dfCombinedFirst.isna()\n",
    "print(blanks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Duplicated Columns or Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Series.count of 0        False\n",
      "1        False\n",
      "2        False\n",
      "3        False\n",
      "4        False\n",
      "         ...  \n",
      "20456    False\n",
      "20457    False\n",
      "20458    False\n",
      "20459    False\n",
      "20460    False\n",
      "Length: 20461, dtype: bool>\n"
     ]
    }
   ],
   "source": [
    "duplicates = dfCombinedFirst.duplicated().count\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing repeated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming a repeating column (PT_TYPE) to PT_NAME\n",
    "dfCombinedFirst = dfCombinedFirst.rename(columns={'PT_TYPE': 'PT_NAME'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Station Codes with Station Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map Station Names (PT_NAME) with Station Codes (PT_CODE)\n",
    "dfCombinedFirst['PT_CODE_FirstPart'] = dfCombinedFirst['PT_CODE'].str.split('/').str[0] #Splitting Stations with multiple codes\n",
    "csv_df = pd.read_csv('./datasets/TrainStationCodes.csv') # Our Train Station Names file\n",
    "code_name_mapping = dict(zip(csv_df['stn_code'], csv_df['mrt_station_english'])) #Mapping\n",
    "dfCombinedFirst['PT_NAME'] = dfCombinedFirst['PT_CODE_FirstPart'].map(code_name_mapping) # Mapping\n",
    "dfCombinedFirst = dfCombinedFirst.drop('PT_CODE_FirstPart', axis=1) # Remove column used for mapping\n",
    "\n",
    "\n",
    "print(dfCombinedFirst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Month_Year & Time_Per_Year column to proper datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert YEAR_MONTH to the last day of the month\n",
    "dfCombinedFirst['YEAR_MONTH'] = pd.to_datetime(dfCombinedFirst['YEAR_MONTH']).dt.to_period('M').dt.to_timestamp('M') + pd.offsets.MonthEnd(0)\n",
    "\n",
    "# Combine with TIME_PER_HOUR to create a full datetime\n",
    "dfCombinedFirst['DATETIME'] = dfCombinedFirst.apply(lambda row: pd.Timestamp(year=row['YEAR_MONTH'].year,\n",
    "                                                   month=row['YEAR_MONTH'].month,\n",
    "                                                   day=row['YEAR_MONTH'].day,\n",
    "                                                   hour=row['TIME_PER_HOUR']), axis=1)\n",
    "\n",
    "# Drop the original YEAR_MONTH column\n",
    "dfCombinedFirst.drop('YEAR_MONTH', axis=1, inplace=True)\n",
    "\n",
    "# Make DATETIME the first column by creating a new DataFrame with the desired column order\n",
    "dfCombinedFirst = dfCombinedFirst[['DATETIME'] + [col for col in dfCombinedFirst.columns if col != 'DATETIME']]\n",
    "\n",
    "# Drop the origin Time_Per_Hour columns\n",
    "dfCombinedFirst.drop('TIME_PER_HOUR', axis=1, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing (Weekdays to 0) & (Weekends/Holidays to  1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCombinedFirst['DAY_TYPE'] = dfCombinedFirst['DAY_TYPE'].map({'WEEKDAY': 0, 'WEEKENDS/HOLIDAY': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Train_Lines column to calculate number of train lines in the station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that counts the number of train lines\n",
    "def count_train_lines(pt_code):\n",
    "    if pd.isna(pt_code):\n",
    "        return 0\n",
    "    return pt_code.count('/') + 1\n",
    "\n",
    "# Apply the function to the PT_CODE column and create the TRAIN_LINES column\n",
    "dfCombinedFirst['TRAIN_LINES'] = dfCombinedFirst['PT_CODE'].apply(count_train_lines)\n",
    "\n",
    "# Insert TRAIN_LINES next to PT_CODE column\n",
    "loc = dfCombinedFirst.columns.get_loc('PT_NAME') + 1\n",
    "dfCombinedFirst.insert(loc, 'TRAIN_LINES', dfCombinedFirst.pop('TRAIN_LINES'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping train lines to train codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_line_mapping = {\n",
    "    'EW': 0, # East-West Line\n",
    "    'CG': 0, # East-West Line to Changi Airport\n",
    "    'NS': 1, # North-South Line\n",
    "    'NE': 2, # North-East Line\n",
    "    'CC': 3, # Circle Line\n",
    "    'CE': 3, # Circle Line (Bayfront, Marina Bay)\n",
    "    'DT': 4, # Downtown Line\n",
    "    'TE': 5, # Thomson-East Coast Line\n",
    "    'BP': 6, # Bukit Panjang LRT\n",
    "    'SW': 7, # Sengkang LRT West\n",
    "    'SE': 7, # Sengkang LRT East\n",
    "    'PW': 8, # Punggol LRT West\n",
    "    'PE': 8, # Punggol LRT East\n",
    "}\n",
    "\n",
    "def map_train_codes(pt_code):\n",
    "    # Initialize an empty list to store the mapped train codes\n",
    "    train_codes = []\n",
    "    # Split the pt_code by '/' and iterate over each part\n",
    "    for code in pt_code.split('/'):\n",
    "        # Iterate over each key in the mapping to find a match\n",
    "        for key in train_line_mapping:\n",
    "            # If the key is found at the start of the code segment, append the mapped value\n",
    "            if code.startswith(key):\n",
    "                train_codes.append(train_line_mapping[key])\n",
    "                break  # Break the loop once the match is found\n",
    "    return train_codes\n",
    "\n",
    "# Apply the revised function to the PT_CODE column\n",
    "dfCombinedFirst['TRAIN_CODES'] = dfCombinedFirst['PT_CODE'].apply(map_train_codes)\n",
    "\n",
    "# Convert the TRAIN_CODES column to a list\n",
    "train_codes_list = dfCombinedFirst['TRAIN_CODES'].tolist()\n",
    "\n",
    "# Insert TRAIN_CODES next to TRAIN_LINES\n",
    "loc = dfCombinedFirst.columns.get_loc('TRAIN_LINES') + 1\n",
    "dfCombinedFirst.insert(loc, 'TRAIN_CODES', dfCombinedFirst.pop('TRAIN_CODES'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check number of train stations in Our Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfTrainStations = dfCombinedFirst['PT_NAME'].nunique()\n",
    "print(numberOfTrainStations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check number of train station in our Location Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfLocation = pd.read_csv(\"./datasets/TrainStationLocation.csv\")\n",
    "unique_station_name_count = dfLocation['station_name'].nunique()\n",
    "print(unique_station_name_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that we have locations data for all of our train stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find PT_NAMEs that are not present in the station_name and drop duplicates\n",
    "missing_stations = dfCombinedFirst[~dfCombinedFirst['PT_NAME'].isin(dfLocation['station_name'])]\n",
    "missing_stations_unique = missing_stations.drop_duplicates(subset=['PT_NAME'])\n",
    " \n",
    "# Get the unique missing station names as a list\n",
    "missing_station_names_unique = missing_stations_unique['PT_NAME'].tolist()\n",
    "\n",
    "print(list(missing_station_names_unique))\n",
    "print(\"This list shows what stations is not in our location dataset.\")\n",
    "print(len(missing_station_names_unique))\n",
    "print(\"This number should be: 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding out what is the hidden 1 station in Singapore.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfLocation_set = set(dfLocation['station_name'])\n",
    "dfCombinedFirst_set = set(dfCombinedFirst['PT_NAME'])\n",
    "\n",
    "\n",
    "unique_station = dfLocation_set - dfCombinedFirst_set\n",
    "\n",
    "\n",
    "print(unique_station)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping location data to our station names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude_mapping = dict(zip(dfLocation['station_name'], dfLocation['lat'])) #Mapping\n",
    "longitude_mappping = dict(zip(dfLocation['station_name'], dfLocation['lng'])) #Mapping\n",
    "dfCombinedFirst['PT_LATITUDE'] = dfCombinedFirst['PT_NAME'].map(latitude_mapping) # Creating PT_LATITUDE column\n",
    "dfCombinedFirst['PT_LONGITUDE'] = dfCombinedFirst['PT_NAME'].map(longitude_mappping) # Creating PT_LONGITUDE column\n",
    "\n",
    "# Insert PT_LATITUDE & PT_LONGITUDE next to PT_CODE column\n",
    "loc = dfCombinedFirst.columns.get_loc('PT_CODE') + 1\n",
    "dfCombinedFirst.insert(loc, 'PT_LATITUDE', dfCombinedFirst.pop('PT_LATITUDE'))\n",
    "loc1 = dfCombinedFirst.columns.get_loc('PT_CODE') + 2\n",
    "dfCombinedFirst.insert(loc1, 'PT_LONGITUDE', dfCombinedFirst.pop('PT_LONGITUDE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving our First Processed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving our processed dataset as csv\n",
    "output_path = './outputDatasets/trainStationData1.csv'\n",
    "dfCombinedFirst.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Second Dataset (Volume for Origin-Destination Train Station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augOriginData = './datasets/AugOrigin_2023.csv'\n",
    "septOriginData = './datasets/septOrigin_2023.csv'\n",
    "octOriginData = './datasets/octOrigin_2023.csv'\n",
    "\n",
    "dfAugOrigin = pd.read_csv(augOriginData)\n",
    "dfSeptOrigin = pd.read_csv(septOriginData)\n",
    "dfOctOrigin = pd.read_csv(octOriginData)\n",
    "\n",
    "print(dfAugOrigin)\n",
    "print(dfSeptOrigin)\n",
    "print(dfOctOrigin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine and verify all files for second dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine All DataFrames\n",
    "dfCombinedSecond = pd.concat([dfAugOrigin, dfSeptOrigin, dfOctOrigin], ignore_index=True)\n",
    "print(dfCombinedSecond)\n",
    "\n",
    "# To Verify Combination\n",
    "totalRows = dfAugOrigin.shape[0] + dfSeptOrigin.shape[0] + dfOctOrigin.shape[0]\n",
    "print(\"Total Rows should be:\", totalRows)\n",
    "if dfAugOrigin.shape[1] == dfSeptOrigin.shape[1] == dfOctOrigin.shape[1]:\n",
    "    print(\"Number of columns is the same for all three DataFrames:\", dfAugOrigin.shape[1])\n",
    "else:\n",
    "    print(\"Column size is not the same for the DataFrames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Our Second Dataset for Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing repeated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming a repeating column (PT_TYPE) to PT_NAME\n",
    "dfCombinedSecond = dfCombinedSecond.rename(columns={'PT_TYPE': 'ORIGIN_PT_NAME'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Station Codes with Station Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map Station Names (PT_NAME) with Station Codes (PT_CODE)\n",
    "#dfCombinedFirst['PT_CODE_FirstPart'] = dfCombinedFirst['PT_CODE'].str.split('/').str[0] #Splitting Stations with multiple codes\n",
    "dfCombinedSecond['ORIGIN_PT_CODE_FirstPart'] = dfCombinedSecond['ORIGIN_PT_CODE'].str.split('/').str[0]\n",
    "code_name_mapping = dict(zip(csv_df['stn_code'], csv_df['mrt_station_english'])) #Mapping\n",
    "dfCombinedSecond['ORIGIN_PT_NAME'] = dfCombinedSecond['ORIGIN_PT_CODE_FirstPart'].map(code_name_mapping) # Mapping\n",
    "dfCombinedSecond = dfCombinedSecond.drop('ORIGIN_PT_CODE_FirstPart', axis=1) # Remove column used for mapping\n",
    "\n",
    "dfCombinedSecond['DESTINATION_PT_CODE_FirstPart'] = dfCombinedSecond['DESTINATION_PT_CODE'].str.split('/').str[0]\n",
    "code_name_mapping = dict(zip(csv_df['stn_code'], csv_df['mrt_station_english'])) #Mapping\n",
    "dfCombinedSecond['DESTINATION_PT_NAME'] = dfCombinedSecond['DESTINATION_PT_CODE_FirstPart'].map(code_name_mapping) # Mapping\n",
    "dfCombinedSecond = dfCombinedSecond.drop('DESTINATION_PT_CODE_FirstPart', axis=1) # Remove column used for mapping\n",
    "\n",
    "# Insert DESTINATION_PT_NAME next to ORIGIN_PT_CODE column\n",
    "loc = dfCombinedSecond.columns.get_loc('ORIGIN_PT_CODE') + 1\n",
    "dfCombinedSecond.insert(loc, 'DESTINATION_PT_NAME', dfCombinedSecond.pop('DESTINATION_PT_NAME'))\n",
    "\n",
    "print(dfCombinedSecond)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing (Weekdays to 0) & (Weekends/Holidays to  1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCombinedSecond['DAY_TYPE'] = dfCombinedSecond['DAY_TYPE'].map({'WEEKDAY': 0, 'WEEKENDS/HOLIDAY': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Train_Lines column to calculate number of train lines in the station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function from count_train_lines to create ORIGIN_TRAIN_LINES column\n",
    "dfCombinedSecond['ORIGIN_TRAIN_LINES'] = dfCombinedSecond['ORIGIN_PT_CODE'].apply(count_train_lines)\n",
    "\n",
    "# Insert ORIGN_TRAIN_LINES next to ORIGIN_PT_NAME column\n",
    "loc = dfCombinedSecond.columns.get_loc('ORIGIN_PT_NAME') + 1\n",
    "dfCombinedSecond.insert(loc, 'ORIGIN_TRAIN_LINES', dfCombinedSecond.pop('ORIGIN_TRAIN_LINES'))\n",
    "\n",
    "# Apply the function from count_train_lines to create DESTINATION_TRAIN_LINES column\n",
    "dfCombinedSecond['DESTINATION_TRAIN_LINES'] = dfCombinedSecond['DESTINATION_PT_CODE'].apply(count_train_lines)\n",
    "\n",
    "# Insert DESTINATION_TRAIN_LINES next to DESTINATION_PT_NAME column\n",
    "loc1 = dfCombinedSecond.columns.get_loc('DESTINATION_PT_NAME') + 1\n",
    "dfCombinedSecond.insert(loc1, 'DESTINATION_TRAIN_LINES', dfCombinedSecond.pop('DESTINATION_TRAIN_LINES'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Month_Year & Time_Per_Year column to proper datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert YEAR_MONTH to the last day of the month\n",
    "dfCombinedSecond['YEAR_MONTH'] = pd.to_datetime(dfCombinedSecond['YEAR_MONTH']).dt.to_period('M').dt.to_timestamp('M') + pd.offsets.MonthEnd(0)\n",
    "\n",
    "# Combine with TIME_PER_HOUR to create a full datetime\n",
    "dfCombinedSecond['DATETIME'] = dfCombinedSecond.apply(lambda row: pd.Timestamp(year=row['YEAR_MONTH'].year,\n",
    "                                                   month=row['YEAR_MONTH'].month,\n",
    "                                                   day=row['YEAR_MONTH'].day,\n",
    "                                                   hour=row['TIME_PER_HOUR']), axis=1)\n",
    "\n",
    "# Drop the original YEAR_MONTH column\n",
    "dfCombinedSecond.drop('YEAR_MONTH', axis=1, inplace=True)\n",
    "\n",
    "# Make DATETIME the first column by creating a new DataFrame with the desired column order\n",
    "dfCombinedSecond = dfCombinedSecond[['DATETIME'] + [col for col in dfCombinedSecond.columns if col != 'DATETIME']]\n",
    "\n",
    "# Drop the origin Time_Per_Hour columns\n",
    "dfCombinedSecond.drop('TIME_PER_HOUR', axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping location data to our station names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude_mapping = dict(zip(dfLocation['station_name'], dfLocation['lat'])) #Mapping\n",
    "longitude_mappping = dict(zip(dfLocation['station_name'], dfLocation['lng'])) #Mapping\n",
    "\n",
    "# Creating ORIGIN_PT_LATITUDE & ORIGIN_PT_LONGITUDE column\n",
    "dfCombinedSecond['ORIGIN_PT_LATITUDE'] = dfCombinedSecond['ORIGIN_PT_NAME'].map(latitude_mapping) \n",
    "dfCombinedSecond['ORIGIN_PT_LONGITUDE'] = dfCombinedSecond['ORIGIN_PT_NAME'].map(longitude_mappping) \n",
    "\n",
    "# Insert ORIGIN LATITUDE & LATITUDE next to their respective columns\n",
    "locOriginLatitude = dfCombinedSecond.columns.get_loc('ORIGIN_PT_CODE') + 1\n",
    "locOriginLongitude = dfCombinedSecond.columns.get_loc('ORIGIN_PT_CODE') + 2\n",
    "dfCombinedSecond.insert(locOriginLatitude, 'ORIGIN_PT_LATITUDE', dfCombinedSecond.pop('ORIGIN_PT_LATITUDE'))\n",
    "dfCombinedSecond.insert(locOriginLongitude, 'ORIGIN_PT_LONGITUDE', dfCombinedSecond.pop('ORIGIN_PT_LONGITUDE'))\n",
    "\n",
    "\n",
    "\n",
    "# Creating DESTINATION_PT_LATITUDE & DESTINATION_PT_LONGITUDE column\n",
    "dfCombinedSecond['DESTINATION_PT_LATITUDE'] = dfCombinedSecond['ORIGIN_PT_NAME'].map(latitude_mapping) \n",
    "dfCombinedSecond['DESTINATION_PT_LONGITUDE'] = dfCombinedSecond['ORIGIN_PT_NAME'].map(longitude_mappping) \n",
    "\n",
    "\n",
    "# Insert DESTINATION LATITUDE & LATITUDE next to their respective columns\n",
    "locDestLatitude = dfCombinedSecond.columns.get_loc('DESTINATION_PT_CODE') + 1\n",
    "locDestLongitude = dfCombinedSecond.columns.get_loc('DESTINATION_PT_CODE') + 2\n",
    "dfCombinedSecond.insert(locDestLatitude, 'DESTINATION_PT_LATITUDE', dfCombinedSecond.pop('DESTINATION_PT_LATITUDE'))\n",
    "dfCombinedSecond.insert(locDestLongitude, 'DESTINATION_PT_LONGITUDE', dfCombinedSecond.pop('DESTINATION_PT_LONGITUDE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping train lines to train codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_line_mapping = {\n",
    "    'EW': 0, # East-West Line\n",
    "    'CG': 0, # East-West Line to Changi Airport\n",
    "    'NS': 1, # North-South Line\n",
    "    'NE': 2, # North-East Line\n",
    "    'CC': 3, # Circle Line\n",
    "    'CE': 3, # Circle Line (Bayfront, Marina Bay)\n",
    "    'DT': 4, # Downtown Line\n",
    "    'TE': 5, # Thomson-East Coast Line\n",
    "    'BP': 6, # Bukit Panjang LRT\n",
    "    'SW': 7, # Sengkang LRT West\n",
    "    'SE': 7, # Sengkang LRT East\n",
    "    'PW': 8, # Punggol LRT West\n",
    "    'PE': 8, # Punggol LRT East\n",
    "}\n",
    "\n",
    "def map_train_codes(pt_code):\n",
    "    # Initialize an empty list to store the mapped train codes\n",
    "    train_codes = []\n",
    "    # Split the pt_code by '/' and iterate over each part\n",
    "    for code in pt_code.split('/'):\n",
    "        # Iterate over each key in the mapping to find a match\n",
    "        for key in train_line_mapping:\n",
    "            # If the key is found at the start of the code segment, append the mapped value\n",
    "            if code.startswith(key):\n",
    "                train_codes.append(train_line_mapping[key])\n",
    "                break  # Break the loop once the match is found\n",
    "    return train_codes\n",
    "\n",
    "\n",
    "# Apply the revised function to the ORIGIN_PT_CODE column\n",
    "dfCombinedSecond['ORIGIN_TRAIN_CODES'] = dfCombinedSecond['ORIGIN_PT_CODE'].apply(map_train_codes)\n",
    "\n",
    "# Convert the ORIGIN_PT_CODE column to a list\n",
    "train_codes_list = dfCombinedSecond['ORIGIN_TRAIN_CODES'].tolist()\n",
    "\n",
    "# Insert ORIGIN_TRAIN_CODES next to ORIGIN_TRAIN_LINES\n",
    "loc = dfCombinedSecond.columns.get_loc('ORIGIN_TRAIN_LINES') + 1\n",
    "dfCombinedSecond.insert(loc, 'ORIGIN_TRAIN_CODES', dfCombinedSecond.pop('ORIGIN_TRAIN_CODES'))\n",
    "\n",
    "\n",
    "\n",
    "# Apply the revised function to the DESTINATION_PT_CODE column\n",
    "dfCombinedSecond['DESTINATION_TRAIN_CODES'] = dfCombinedSecond['DESTINATION_PT_CODE'].apply(map_train_codes)\n",
    "\n",
    "# Convert the DESTINATION_PT_CODE column to a list\n",
    "train_codes_list = dfCombinedSecond['DESTINATION_TRAIN_CODES'].tolist()\n",
    "\n",
    "# Insert DESTINATION_TRAIN_CODES next to DESTINATION_TRAIN_LINES\n",
    "loc1 = dfCombinedSecond.columns.get_loc('DESTINATION_TRAIN_LINES') + 1\n",
    "dfCombinedSecond.insert(loc1, 'DESTINATION_TRAIN_CODES', dfCombinedSecond.pop('DESTINATION_TRAIN_CODES'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving our Second Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving our processed dataset as csv\n",
    "output_path = './outputDatasets/trainStationData2.csv'\n",
    "dfCombinedSecond.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End of processing,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving onto trainDataAnalysis!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
