{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting our AWS Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(profile_name='personal-ltadatamall')\n",
    "s3 = session.resource('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import First Dataset (Volume Per Train Station)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using AWS S3 Bucket for our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      YEAR_MONTH          DAY_TYPE  TIME_PER_HOUR PT_TYPE        PT_CODE  \\\n",
      "0        2023-08           WEEKDAY             22   TRAIN           NS28   \n",
      "1        2023-08  WEEKENDS/HOLIDAY             22   TRAIN           NS28   \n",
      "2        2023-08  WEEKENDS/HOLIDAY              0   TRAIN      DT10/TE11   \n",
      "3        2023-08           WEEKDAY              0   TRAIN      DT10/TE11   \n",
      "4        2023-08           WEEKDAY             10   TRAIN  EW16/NE3/TE17   \n",
      "...          ...               ...            ...     ...            ...   \n",
      "34087    2023-12  WEEKENDS/HOLIDAY              6   TRAIN           DT23   \n",
      "34088    2023-12           WEEKDAY              7   TRAIN  NS27/CE2/TE20   \n",
      "34089    2023-12  WEEKENDS/HOLIDAY              7   TRAIN  NS27/CE2/TE20   \n",
      "34090    2023-12           WEEKDAY             12   TRAIN            SE5   \n",
      "34091    2023-12  WEEKENDS/HOLIDAY             12   TRAIN            SE5   \n",
      "\n",
      "       TOTAL_TAP_IN_VOLUME  TOTAL_TAP_OUT_VOLUME  \n",
      "0                      752                   311  \n",
      "1                      612                   223  \n",
      "2                       37                   242  \n",
      "3                       86                   445  \n",
      "4                    28179                 39454  \n",
      "...                    ...                   ...  \n",
      "34087                  459                   485  \n",
      "34088                  918                  6227  \n",
      "34089                  446                  1100  \n",
      "34090                 2734                  1822  \n",
      "34091                 1809                  1143  \n",
      "\n",
      "[34092 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "bucket = s3.Bucket('ltadatamall')\n",
    "\n",
    "dfCombinedFirst = pd.DataFrame()\n",
    "\n",
    "for obj in bucket.objects.filter(Prefix='TrainVolume_Data/'):\n",
    "    if not obj.key.endswith('/'):\n",
    "        # Use the object's get method to retrieve the object\n",
    "        response = obj.get()\n",
    "        df_temp = pd.read_csv(response['Body'])  # Assuming the file is a CSV\n",
    "        dfCombinedFirst = pd.concat([dfCombinedFirst, df_temp], ignore_index=True)\n",
    "\n",
    "print(dfCombinedFirst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Our First Dataset for Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Blank Columns or Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       YEAR_MONTH  DAY_TYPE  TIME_PER_HOUR  PT_TYPE  PT_CODE  \\\n",
      "0           False     False          False    False    False   \n",
      "1           False     False          False    False    False   \n",
      "2           False     False          False    False    False   \n",
      "3           False     False          False    False    False   \n",
      "4           False     False          False    False    False   \n",
      "...           ...       ...            ...      ...      ...   \n",
      "34087       False     False          False    False    False   \n",
      "34088       False     False          False    False    False   \n",
      "34089       False     False          False    False    False   \n",
      "34090       False     False          False    False    False   \n",
      "34091       False     False          False    False    False   \n",
      "\n",
      "       TOTAL_TAP_IN_VOLUME  TOTAL_TAP_OUT_VOLUME  \n",
      "0                    False                 False  \n",
      "1                    False                 False  \n",
      "2                    False                 False  \n",
      "3                    False                 False  \n",
      "4                    False                 False  \n",
      "...                    ...                   ...  \n",
      "34087                False                 False  \n",
      "34088                False                 False  \n",
      "34089                False                 False  \n",
      "34090                False                 False  \n",
      "34091                False                 False  \n",
      "\n",
      "[34092 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Using isna() method to find blanks\n",
    "blanks = dfCombinedFirst.isna()\n",
    "print(blanks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Duplicated Columns or Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Series.count of 0        False\n",
      "1        False\n",
      "2        False\n",
      "3        False\n",
      "4        False\n",
      "         ...  \n",
      "34087    False\n",
      "34088    False\n",
      "34089    False\n",
      "34090    False\n",
      "34091    False\n",
      "Length: 34092, dtype: bool>\n"
     ]
    }
   ],
   "source": [
    "duplicates = dfCombinedFirst.duplicated().count\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing repeated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming a repeating column (PT_TYPE) to PT_NAME\n",
    "dfCombinedFirst = dfCombinedFirst.rename(columns={'PT_TYPE': 'PT_NAME'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Station Codes with Station Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      YEAR_MONTH          DAY_TYPE  TIME_PER_HOUR            PT_NAME  \\\n",
      "0        2023-08           WEEKDAY             22  Marina South Pier   \n",
      "1        2023-08  WEEKENDS/HOLIDAY             22  Marina South Pier   \n",
      "2        2023-08  WEEKENDS/HOLIDAY              0            Stevens   \n",
      "3        2023-08           WEEKDAY              0            Stevens   \n",
      "4        2023-08           WEEKDAY             10        Outram Park   \n",
      "...          ...               ...            ...                ...   \n",
      "34087    2023-12  WEEKENDS/HOLIDAY              6          Bendemeer   \n",
      "34088    2023-12           WEEKDAY              7         Marina Bay   \n",
      "34089    2023-12  WEEKENDS/HOLIDAY              7         Marina Bay   \n",
      "34090    2023-12           WEEKDAY             12           Ranggung   \n",
      "34091    2023-12  WEEKENDS/HOLIDAY             12           Ranggung   \n",
      "\n",
      "             PT_CODE  TOTAL_TAP_IN_VOLUME  TOTAL_TAP_OUT_VOLUME  \n",
      "0               NS28                  752                   311  \n",
      "1               NS28                  612                   223  \n",
      "2          DT10/TE11                   37                   242  \n",
      "3          DT10/TE11                   86                   445  \n",
      "4      EW16/NE3/TE17                28179                 39454  \n",
      "...              ...                  ...                   ...  \n",
      "34087           DT23                  459                   485  \n",
      "34088  NS27/CE2/TE20                  918                  6227  \n",
      "34089  NS27/CE2/TE20                  446                  1100  \n",
      "34090            SE5                 2734                  1822  \n",
      "34091            SE5                 1809                  1143  \n",
      "\n",
      "[34092 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Map Station Names (PT_NAME) with Station Codes (PT_CODE)\n",
    "dfCombinedFirst['PT_CODE_FirstPart'] = dfCombinedFirst['PT_CODE'].str.split('/').str[0] #Splitting Stations with multiple codes\n",
    "csv_df = pd.read_csv(s3.Object('ltadatamall', 'TrainStationCodes.csv').get()['Body']) # Our Train Station Code File from AWS S3 Bucket\n",
    "code_name_mapping = dict(zip(csv_df['stn_code'], csv_df['mrt_station_english'])) #Mapping\n",
    "dfCombinedFirst['PT_NAME'] = dfCombinedFirst['PT_CODE_FirstPart'].map(code_name_mapping) # Mapping\n",
    "dfCombinedFirst = dfCombinedFirst.drop('PT_CODE_FirstPart', axis=1) # Remove column used for mapping\n",
    "\n",
    "\n",
    "print(dfCombinedFirst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Month_Year & Time_Per_Year column to proper datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert YEAR_MONTH to the last day of the month\n",
    "dfCombinedFirst['YEAR_MONTH'] = pd.to_datetime(dfCombinedFirst['YEAR_MONTH']).dt.to_period('M').dt.to_timestamp('M') + pd.offsets.MonthEnd(0)\n",
    "\n",
    "# Combine with TIME_PER_HOUR to create a full datetime\n",
    "dfCombinedFirst['DATETIME'] = dfCombinedFirst.apply(lambda row: pd.Timestamp(year=row['YEAR_MONTH'].year,\n",
    "                                                   month=row['YEAR_MONTH'].month,\n",
    "                                                   day=row['YEAR_MONTH'].day,\n",
    "                                                   hour=row['TIME_PER_HOUR']), axis=1)\n",
    "\n",
    "# Drop the original YEAR_MONTH column\n",
    "dfCombinedFirst.drop('YEAR_MONTH', axis=1, inplace=True)\n",
    "\n",
    "# Make DATETIME the first column by creating a new DataFrame with the desired column order\n",
    "dfCombinedFirst = dfCombinedFirst[['DATETIME'] + [col for col in dfCombinedFirst.columns if col != 'DATETIME']]\n",
    "\n",
    "# Drop the origin Time_Per_Hour columns\n",
    "dfCombinedFirst.drop('TIME_PER_HOUR', axis=1, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing (Weekdays to 0) & (Weekends/Holidays to  1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCombinedFirst['DAY_TYPE'] = dfCombinedFirst['DAY_TYPE'].map({'WEEKDAY': 0, 'WEEKENDS/HOLIDAY': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Train_Lines column to calculate number of train lines in the station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that counts the number of train lines\n",
    "def count_train_lines(pt_code):\n",
    "    if pd.isna(pt_code):\n",
    "        return 0\n",
    "    return pt_code.count('/') + 1\n",
    "\n",
    "# Apply the function to the PT_CODE column and create the TRAIN_LINES column\n",
    "dfCombinedFirst['TRAIN_LINES'] = dfCombinedFirst['PT_CODE'].apply(count_train_lines)\n",
    "\n",
    "# Insert TRAIN_LINES next to PT_CODE column\n",
    "loc = dfCombinedFirst.columns.get_loc('PT_NAME') + 1\n",
    "dfCombinedFirst.insert(loc, 'TRAIN_LINES', dfCombinedFirst.pop('TRAIN_LINES'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping train lines to train codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_line_mapping = {\n",
    "    'EW': 0, # East-West Line\n",
    "    'CG': 0, # East-West Line to Changi Airport\n",
    "    'NS': 1, # North-South Line\n",
    "    'NE': 2, # North-East Line\n",
    "    'CC': 3, # Circle Line\n",
    "    'CE': 3, # Circle Line (Bayfront, Marina Bay)\n",
    "    'DT': 4, # Downtown Line\n",
    "    'TE': 5, # Thomson-East Coast Line\n",
    "    'BP': 6, # Bukit Panjang LRT\n",
    "    'SW': 7, # Sengkang LRT West\n",
    "    'SE': 7, # Sengkang LRT East\n",
    "    'PW': 8, # Punggol LRT West\n",
    "    'PE': 8, # Punggol LRT East\n",
    "}\n",
    "\n",
    "def map_train_codes(pt_code):\n",
    "    # Initialize an empty list to store the mapped train codes\n",
    "    train_codes = []\n",
    "    # Split the pt_code by '/' and iterate over each part\n",
    "    for code in pt_code.split('/'):\n",
    "        # Iterate over each key in the mapping to find a match\n",
    "        for key in train_line_mapping:\n",
    "            # If the key is found at the start of the code segment, append the mapped value\n",
    "            if code.startswith(key):\n",
    "                train_codes.append(train_line_mapping[key])\n",
    "                break  # Break the loop once the match is found\n",
    "    return train_codes\n",
    "\n",
    "# Apply the revised function to the PT_CODE column\n",
    "dfCombinedFirst['TRAIN_CODES'] = dfCombinedFirst['PT_CODE'].apply(map_train_codes)\n",
    "\n",
    "# Convert the TRAIN_CODES column to a list\n",
    "train_codes_list = dfCombinedFirst['TRAIN_CODES'].tolist()\n",
    "\n",
    "# Insert TRAIN_CODES next to TRAIN_LINES\n",
    "loc = dfCombinedFirst.columns.get_loc('TRAIN_LINES') + 1\n",
    "dfCombinedFirst.insert(loc, 'TRAIN_CODES', dfCombinedFirst.pop('TRAIN_CODES'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check number of train stations in Our Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171\n"
     ]
    }
   ],
   "source": [
    "numberOfTrainStations = dfCombinedFirst['PT_NAME'].nunique()\n",
    "print(numberOfTrainStations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check number of train station in our Location Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172\n"
     ]
    }
   ],
   "source": [
    "dfLocation = pd.read_csv(s3.Object('ltadatamall', 'TrainStationLocation.csv').get()['Body']) # Our Train Station Location File from AWS S3 Bucket\n",
    "unique_station_name_count = dfLocation['station_name'].nunique()\n",
    "print(unique_station_name_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that we have locations data for all of our train stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "This list shows what stations is not in our location dataset.\n",
      "0\n",
      "This number should be: 0\n"
     ]
    }
   ],
   "source": [
    "# Find PT_NAMEs that are not present in the station_name and drop duplicates\n",
    "missing_stations = dfCombinedFirst[~dfCombinedFirst['PT_NAME'].isin(dfLocation['station_name'])]\n",
    "missing_stations_unique = missing_stations.drop_duplicates(subset=['PT_NAME'])\n",
    " \n",
    "# Get the unique missing station names as a list\n",
    "missing_station_names_unique = missing_stations_unique['PT_NAME'].tolist()\n",
    "\n",
    "print(list(missing_station_names_unique))\n",
    "print(\"This list shows what stations is not in our location dataset.\")\n",
    "print(len(missing_station_names_unique))\n",
    "print(\"This number should be: 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding out what is the hidden 1 station in Singapore.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Teck Lee'}\n"
     ]
    }
   ],
   "source": [
    "dfLocation_set = set(dfLocation['station_name'])\n",
    "dfCombinedFirst_set = set(dfCombinedFirst['PT_NAME'])\n",
    "\n",
    "\n",
    "unique_station = dfLocation_set - dfCombinedFirst_set\n",
    "\n",
    "\n",
    "print(unique_station)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping location data to our station names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude_mapping = dict(zip(dfLocation['station_name'], dfLocation['lat'])) #Mapping\n",
    "longitude_mappping = dict(zip(dfLocation['station_name'], dfLocation['lng'])) #Mapping\n",
    "dfCombinedFirst['PT_LATITUDE'] = dfCombinedFirst['PT_NAME'].map(latitude_mapping) # Creating PT_LATITUDE column\n",
    "dfCombinedFirst['PT_LONGITUDE'] = dfCombinedFirst['PT_NAME'].map(longitude_mappping) # Creating PT_LONGITUDE column\n",
    "\n",
    "# Insert PT_LATITUDE & PT_LONGITUDE next to PT_CODE column\n",
    "loc = dfCombinedFirst.columns.get_loc('PT_CODE') + 1\n",
    "dfCombinedFirst.insert(loc, 'PT_LATITUDE', dfCombinedFirst.pop('PT_LATITUDE'))\n",
    "loc1 = dfCombinedFirst.columns.get_loc('PT_CODE') + 2\n",
    "dfCombinedFirst.insert(loc1, 'PT_LONGITUDE', dfCombinedFirst.pop('PT_LONGITUDE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving our First Processed Dataset on AWS S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded successfully to AWS S3.\n"
     ]
    }
   ],
   "source": [
    "# Convert our Dataframe to CSV file format\n",
    "df_to_csv = dfCombinedFirst.to_csv(index=False)\n",
    "\n",
    "# Get the S3 object\n",
    "s3_object = s3.Object('ltadatamall', 'TrainVolume_ProcessedData/Data.csv')\n",
    "\n",
    "# Write our CSV data to AWS S3 \n",
    "s3_object.put(Body=df_to_csv)\n",
    "\n",
    "print(\"Data uploaded successfully to AWS S3.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Second Dataset (Volume for Origin-Destination Train Station)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using AWS S3 Bucket for our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        YEAR_MONTH          DAY_TYPE  TIME_PER_HOUR PT_TYPE ORIGIN_PT_CODE  \\\n",
      "0          2023-08           WEEKDAY             13   TRAIN           NE11   \n",
      "1          2023-08  WEEKENDS/HOLIDAY             13   TRAIN           NS19   \n",
      "2          2023-08           WEEKDAY             13   TRAIN           NS19   \n",
      "3          2023-08  WEEKENDS/HOLIDAY             13   TRAIN           NE11   \n",
      "4          2023-08  WEEKENDS/HOLIDAY             14   TRAIN       CC4/DT15   \n",
      "...            ...               ...            ...     ...            ...   \n",
      "4022728    2023-12  WEEKENDS/HOLIDAY             10   TRAIN            NS2   \n",
      "4022729    2023-12  WEEKENDS/HOLIDAY             22   TRAIN      EW21/CC22   \n",
      "4022730    2023-12           WEEKDAY             22   TRAIN           DT17   \n",
      "4022731    2023-12           WEEKDAY             22   TRAIN      EW21/CC22   \n",
      "4022732    2023-12  WEEKENDS/HOLIDAY             22   TRAIN           DT17   \n",
      "\n",
      "        DESTINATION_PT_CODE  TOTAL_TRIPS  \n",
      "0                      NS19           36  \n",
      "1                      NE11           11  \n",
      "2                      NE11           25  \n",
      "3                      NS19           17  \n",
      "4                       NS8            2  \n",
      "...                     ...          ...  \n",
      "4022728                TE12            2  \n",
      "4022729                DT17            5  \n",
      "4022730           EW21/CC22           12  \n",
      "4022731                DT17            3  \n",
      "4022732           EW21/CC22            3  \n",
      "\n",
      "[4022733 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "bucket = s3.Bucket('ltadatamall')\n",
    "\n",
    "dfCombinedSecond = pd.DataFrame()\n",
    "\n",
    "for obj in bucket.objects.filter(Prefix='TrainVolumeOrigin_Data/'):\n",
    "    if not obj.key.endswith('/'):\n",
    "        # Use the object's get method to retrieve the object\n",
    "        response = obj.get()\n",
    "        df_temp = pd.read_csv(response['Body'])  # Assuming the file is a CSV\n",
    "        dfCombinedSecond = pd.concat([dfCombinedSecond, df_temp], ignore_index=True)\n",
    "\n",
    "print(dfCombinedSecond)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Our Second Dataset for Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing repeated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming a repeating column (PT_TYPE) to PT_NAME\n",
    "dfCombinedSecond = dfCombinedSecond.rename(columns={'PT_TYPE': 'ORIGIN_PT_NAME'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Station Codes with Station Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        YEAR_MONTH          DAY_TYPE  TIME_PER_HOUR ORIGIN_PT_NAME  \\\n",
      "0          2023-08           WEEKDAY             13      Woodleigh   \n",
      "1          2023-08  WEEKENDS/HOLIDAY             13      Toa Payoh   \n",
      "2          2023-08           WEEKDAY             13      Toa Payoh   \n",
      "3          2023-08  WEEKENDS/HOLIDAY             13      Woodleigh   \n",
      "4          2023-08  WEEKENDS/HOLIDAY             14      Promenade   \n",
      "...            ...               ...            ...            ...   \n",
      "4022728    2023-12  WEEKENDS/HOLIDAY             10    Bukit Batok   \n",
      "4022729    2023-12  WEEKENDS/HOLIDAY             22    Buona Vista   \n",
      "4022730    2023-12           WEEKDAY             22       Downtown   \n",
      "4022731    2023-12           WEEKDAY             22    Buona Vista   \n",
      "4022732    2023-12  WEEKENDS/HOLIDAY             22       Downtown   \n",
      "\n",
      "        ORIGIN_PT_CODE DESTINATION_PT_NAME DESTINATION_PT_CODE  TOTAL_TRIPS  \n",
      "0                 NE11           Toa Payoh                NS19           36  \n",
      "1                 NS19           Woodleigh                NE11           11  \n",
      "2                 NS19           Woodleigh                NE11           25  \n",
      "3                 NE11           Toa Payoh                NS19           17  \n",
      "4             CC4/DT15           Marsiling                 NS8            2  \n",
      "...                ...                 ...                 ...          ...  \n",
      "4022728            NS2              Napier                TE12            2  \n",
      "4022729      EW21/CC22            Downtown                DT17            5  \n",
      "4022730           DT17         Buona Vista           EW21/CC22           12  \n",
      "4022731      EW21/CC22            Downtown                DT17            3  \n",
      "4022732           DT17         Buona Vista           EW21/CC22            3  \n",
      "\n",
      "[4022733 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Map Station Names (PT_NAME) with Station Codes (PT_CODE)\n",
    "#dfCombinedFirst['PT_CODE_FirstPart'] = dfCombinedFirst['PT_CODE'].str.split('/').str[0] #Splitting Stations with multiple codes\n",
    "dfCombinedSecond['ORIGIN_PT_CODE_FirstPart'] = dfCombinedSecond['ORIGIN_PT_CODE'].str.split('/').str[0]\n",
    "code_name_mapping = dict(zip(csv_df['stn_code'], csv_df['mrt_station_english'])) #Mapping\n",
    "dfCombinedSecond['ORIGIN_PT_NAME'] = dfCombinedSecond['ORIGIN_PT_CODE_FirstPart'].map(code_name_mapping) # Mapping\n",
    "dfCombinedSecond = dfCombinedSecond.drop('ORIGIN_PT_CODE_FirstPart', axis=1) # Remove column used for mapping\n",
    "\n",
    "dfCombinedSecond['DESTINATION_PT_CODE_FirstPart'] = dfCombinedSecond['DESTINATION_PT_CODE'].str.split('/').str[0]\n",
    "code_name_mapping = dict(zip(csv_df['stn_code'], csv_df['mrt_station_english'])) #Mapping\n",
    "dfCombinedSecond['DESTINATION_PT_NAME'] = dfCombinedSecond['DESTINATION_PT_CODE_FirstPart'].map(code_name_mapping) # Mapping\n",
    "dfCombinedSecond = dfCombinedSecond.drop('DESTINATION_PT_CODE_FirstPart', axis=1) # Remove column used for mapping\n",
    "\n",
    "# Insert DESTINATION_PT_NAME next to ORIGIN_PT_CODE column\n",
    "loc = dfCombinedSecond.columns.get_loc('ORIGIN_PT_CODE') + 1\n",
    "dfCombinedSecond.insert(loc, 'DESTINATION_PT_NAME', dfCombinedSecond.pop('DESTINATION_PT_NAME'))\n",
    "\n",
    "print(dfCombinedSecond)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing (Weekdays to 0) & (Weekends/Holidays to  1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCombinedSecond['DAY_TYPE'] = dfCombinedSecond['DAY_TYPE'].map({'WEEKDAY': 0, 'WEEKENDS/HOLIDAY': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Train_Lines column to calculate number of train lines in the station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function from count_train_lines to create ORIGIN_TRAIN_LINES column\n",
    "dfCombinedSecond['ORIGIN_TRAIN_LINES'] = dfCombinedSecond['ORIGIN_PT_CODE'].apply(count_train_lines)\n",
    "\n",
    "# Insert ORIGN_TRAIN_LINES next to ORIGIN_PT_NAME column\n",
    "loc = dfCombinedSecond.columns.get_loc('ORIGIN_PT_NAME') + 1\n",
    "dfCombinedSecond.insert(loc, 'ORIGIN_TRAIN_LINES', dfCombinedSecond.pop('ORIGIN_TRAIN_LINES'))\n",
    "\n",
    "# Apply the function from count_train_lines to create DESTINATION_TRAIN_LINES column\n",
    "dfCombinedSecond['DESTINATION_TRAIN_LINES'] = dfCombinedSecond['DESTINATION_PT_CODE'].apply(count_train_lines)\n",
    "\n",
    "# Insert DESTINATION_TRAIN_LINES next to DESTINATION_PT_NAME column\n",
    "loc1 = dfCombinedSecond.columns.get_loc('DESTINATION_PT_NAME') + 1\n",
    "dfCombinedSecond.insert(loc1, 'DESTINATION_TRAIN_LINES', dfCombinedSecond.pop('DESTINATION_TRAIN_LINES'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Month_Year & Time_Per_Year column to proper datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert YEAR_MONTH to the last day of the month\n",
    "dfCombinedSecond['YEAR_MONTH'] = pd.to_datetime(dfCombinedSecond['YEAR_MONTH']).dt.to_period('M').dt.to_timestamp('M') + pd.offsets.MonthEnd(0)\n",
    "\n",
    "# Combine with TIME_PER_HOUR to create a full datetime\n",
    "dfCombinedSecond['DATETIME'] = dfCombinedSecond.apply(lambda row: pd.Timestamp(year=row['YEAR_MONTH'].year,\n",
    "                                                   month=row['YEAR_MONTH'].month,\n",
    "                                                   day=row['YEAR_MONTH'].day,\n",
    "                                                   hour=row['TIME_PER_HOUR']), axis=1)\n",
    "\n",
    "# Drop the original YEAR_MONTH column\n",
    "dfCombinedSecond.drop('YEAR_MONTH', axis=1, inplace=True)\n",
    "\n",
    "# Make DATETIME the first column by creating a new DataFrame with the desired column order\n",
    "dfCombinedSecond = dfCombinedSecond[['DATETIME'] + [col for col in dfCombinedSecond.columns if col != 'DATETIME']]\n",
    "\n",
    "# Drop the origin Time_Per_Hour columns\n",
    "dfCombinedSecond.drop('TIME_PER_HOUR', axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping location data to our station names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude_mapping = dict(zip(dfLocation['station_name'], dfLocation['lat'])) #Mapping\n",
    "longitude_mappping = dict(zip(dfLocation['station_name'], dfLocation['lng'])) #Mapping\n",
    "\n",
    "# Creating ORIGIN_PT_LATITUDE & ORIGIN_PT_LONGITUDE column\n",
    "dfCombinedSecond['ORIGIN_PT_LATITUDE'] = dfCombinedSecond['ORIGIN_PT_NAME'].map(latitude_mapping) \n",
    "dfCombinedSecond['ORIGIN_PT_LONGITUDE'] = dfCombinedSecond['ORIGIN_PT_NAME'].map(longitude_mappping) \n",
    "\n",
    "# Insert ORIGIN LATITUDE & LATITUDE next to their respective columns\n",
    "locOriginLatitude = dfCombinedSecond.columns.get_loc('ORIGIN_PT_CODE') + 1\n",
    "locOriginLongitude = dfCombinedSecond.columns.get_loc('ORIGIN_PT_CODE') + 2\n",
    "dfCombinedSecond.insert(locOriginLatitude, 'ORIGIN_PT_LATITUDE', dfCombinedSecond.pop('ORIGIN_PT_LATITUDE'))\n",
    "dfCombinedSecond.insert(locOriginLongitude, 'ORIGIN_PT_LONGITUDE', dfCombinedSecond.pop('ORIGIN_PT_LONGITUDE'))\n",
    "\n",
    "\n",
    "\n",
    "# Creating DESTINATION_PT_LATITUDE & DESTINATION_PT_LONGITUDE column\n",
    "dfCombinedSecond['DESTINATION_PT_LATITUDE'] = dfCombinedSecond['ORIGIN_PT_NAME'].map(latitude_mapping) \n",
    "dfCombinedSecond['DESTINATION_PT_LONGITUDE'] = dfCombinedSecond['ORIGIN_PT_NAME'].map(longitude_mappping) \n",
    "\n",
    "\n",
    "# Insert DESTINATION LATITUDE & LATITUDE next to their respective columns\n",
    "locDestLatitude = dfCombinedSecond.columns.get_loc('DESTINATION_PT_CODE') + 1\n",
    "locDestLongitude = dfCombinedSecond.columns.get_loc('DESTINATION_PT_CODE') + 2\n",
    "dfCombinedSecond.insert(locDestLatitude, 'DESTINATION_PT_LATITUDE', dfCombinedSecond.pop('DESTINATION_PT_LATITUDE'))\n",
    "dfCombinedSecond.insert(locDestLongitude, 'DESTINATION_PT_LONGITUDE', dfCombinedSecond.pop('DESTINATION_PT_LONGITUDE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping train lines to train codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_line_mapping = {\n",
    "    'EW': 0, # East-West Line\n",
    "    'CG': 0, # East-West Line to Changi Airport\n",
    "    'NS': 1, # North-South Line\n",
    "    'NE': 2, # North-East Line\n",
    "    'CC': 3, # Circle Line\n",
    "    'CE': 3, # Circle Line (Bayfront, Marina Bay)\n",
    "    'DT': 4, # Downtown Line\n",
    "    'TE': 5, # Thomson-East Coast Line\n",
    "    'BP': 6, # Bukit Panjang LRT\n",
    "    'SW': 7, # Sengkang LRT West\n",
    "    'SE': 7, # Sengkang LRT East\n",
    "    'PW': 8, # Punggol LRT West\n",
    "    'PE': 8, # Punggol LRT East\n",
    "}\n",
    "\n",
    "def map_train_codes(pt_code):\n",
    "    # Initialize an empty list to store the mapped train codes\n",
    "    train_codes = []\n",
    "    # Split the pt_code by '/' and iterate over each part\n",
    "    for code in pt_code.split('/'):\n",
    "        # Iterate over each key in the mapping to find a match\n",
    "        for key in train_line_mapping:\n",
    "            # If the key is found at the start of the code segment, append the mapped value\n",
    "            if code.startswith(key):\n",
    "                train_codes.append(train_line_mapping[key])\n",
    "                break  # Break the loop once the match is found\n",
    "    return train_codes\n",
    "\n",
    "\n",
    "# Apply the revised function to the ORIGIN_PT_CODE column\n",
    "dfCombinedSecond['ORIGIN_TRAIN_CODES'] = dfCombinedSecond['ORIGIN_PT_CODE'].apply(map_train_codes)\n",
    "\n",
    "# Convert the ORIGIN_PT_CODE column to a list\n",
    "train_codes_list = dfCombinedSecond['ORIGIN_TRAIN_CODES'].tolist()\n",
    "\n",
    "# Insert ORIGIN_TRAIN_CODES next to ORIGIN_TRAIN_LINES\n",
    "loc = dfCombinedSecond.columns.get_loc('ORIGIN_TRAIN_LINES') + 1\n",
    "dfCombinedSecond.insert(loc, 'ORIGIN_TRAIN_CODES', dfCombinedSecond.pop('ORIGIN_TRAIN_CODES'))\n",
    "\n",
    "\n",
    "\n",
    "# Apply the revised function to the DESTINATION_PT_CODE column\n",
    "dfCombinedSecond['DESTINATION_TRAIN_CODES'] = dfCombinedSecond['DESTINATION_PT_CODE'].apply(map_train_codes)\n",
    "\n",
    "# Convert the DESTINATION_PT_CODE column to a list\n",
    "train_codes_list = dfCombinedSecond['DESTINATION_TRAIN_CODES'].tolist()\n",
    "\n",
    "# Insert DESTINATION_TRAIN_CODES next to DESTINATION_TRAIN_LINES\n",
    "loc1 = dfCombinedSecond.columns.get_loc('DESTINATION_TRAIN_LINES') + 1\n",
    "dfCombinedSecond.insert(loc1, 'DESTINATION_TRAIN_CODES', dfCombinedSecond.pop('DESTINATION_TRAIN_CODES'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving our Second Processed Dataset onto AWS S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded successfully to AWS S3.\n"
     ]
    }
   ],
   "source": [
    "# Convert our Dataframe to CSV file format\n",
    "df_to_csv = dfCombinedSecond.to_csv(index=False) \n",
    "\n",
    "# Get our S3 object\n",
    "s3_object = s3.Object('ltadatamall', 'TrainVolumeOrigin_ProcessedData/Data.csv')\n",
    "\n",
    "# Write our CSV data to AWS S3 \n",
    "s3_object.put(Body=df_to_csv)\n",
    "\n",
    "print(\"Data uploaded successfully to AWS S3.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End of processing,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving onto trainDataAnalysis!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
